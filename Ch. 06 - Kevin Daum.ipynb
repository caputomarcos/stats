{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems 6-8, 22, 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "from statsmodels import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6\n",
    "\n",
    "### a. \n",
    "\n",
    "\n",
    "$\\alpha=0.01$\n",
    "\n",
    "$H_0: \\beta_1 = \\beta_2 = 0$\n",
    "\n",
    "$H_a:$ not all $\\beta$ equal zero\n",
    "\n",
    "$F^* = \\frac{MSR}{MSE}$\n",
    "\n",
    "If $F^* \\le F(1-\\alpha; p-1; n-p)$, conclude $H_0$\n",
    "\n",
    "If $F^* > F(1-\\alpha; p-1; n-p)$, conclude $H_a$\n",
    "\n",
    "In this case, $p=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_liking</th>\n",
       "      <th>moisture</th>\n",
       "      <th>sweetness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_liking  moisture  sweetness\n",
       "0            64         4          2\n",
       "1            73         4          4\n",
       "2            61         4          2\n",
       "3            76         4          4\n",
       "4            72         6          2"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the brand preference data\n",
    "df_bp = pd.read_table('/Users/kevin/Dropbox/School/STA-580/ch6hw/CH06PR05.txt',\n",
    "                     sep='\\s*', index_col=False, engine='python',\n",
    "                     names=['brand_liking', 'moisture', 'sweetness'])\n",
    "df_bp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `statsmodels` ordinary least squares regression to get $MSE$ and $MSR$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.2538461538461547"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit an OLS model with intercept on moisture and sweetness\n",
    "X_bp = df_bp[['moisture', 'sweetness']]\n",
    "X_bp = sm.add_constant(X_bp)\n",
    "y_bp = df_bp['brand_liking']\n",
    "lr_bp = sm.OLS(y_bp, X_bp).fit()\n",
    "lr_bp.mse_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936.35000000000002"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bp.mse_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I want to check against Minitab:\n",
    "\n",
    "    Source         DF   Seq SS  Contribution   Adj SS   Adj MS  F-Value  P-Value\n",
    "    Regression      2  1872.70        95.21%  1872.70   936.35   129.08    0.000\n",
    "    Error          13    94.30         4.79%    94.30     7.25\n",
    "    \n",
    "Minitab is reporting the same values for $MSE$ and $MSR$. Now, let's calculate $F^*$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.08324496288441"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fstar_bp = lr_bp.mse_model / lr_bp.mse_resid\n",
    "fstar_bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out I can get this F-value and its associated p-value directly from `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.08324496288441"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bp.fvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6582612670820174e-09"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bp.f_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be thorough, let's find $F_{crit} = F(1-\\alpha; p-1; n-p)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.2262352803113856"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcrit_bp = st.f.ppf(0.99,2,lr_bp.nobs)\n",
    "fcrit_bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 129.1 > 6.2, we conclude $H_a$: $\\beta_1$ and $\\beta_2$ do not both equal zero. In other words, at least one of the two variables (moisture and sweetness) are useful in regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. \n",
    "\n",
    "p-value = 2.66e-09\n",
    "\n",
    "### c. \n",
    "\n",
    "First, let's see what `statsmodels` provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>28.624911</td>\n",
       "      <td>46.675089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moisture</th>\n",
       "      <td>3.517944</td>\n",
       "      <td>5.332056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweetness</th>\n",
       "      <td>2.346762</td>\n",
       "      <td>6.403238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0          1\n",
       "const      28.624911  46.675089\n",
       "moisture    3.517944   5.332056\n",
       "sweetness   2.346762   6.403238"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bp.conf_int(alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not know if `statsmodels` uses the Bonferroni method, so I will calculate the intervals manually for comparison. In order to use (6.52), I'll need to calculate B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3724679378582669"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_bp = st.t.ppf(1-.01/(2*2),lr_bp.nobs-3)\n",
    "B_bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the coeffecients and standard errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const        37.650\n",
       "moisture      4.425\n",
       "sweetness     4.375\n",
       "dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coefficients\n",
    "lr_bp.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const        2.996103\n",
       "moisture     0.301120\n",
       "sweetness    0.673324\n",
       "dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard errors\n",
    "lr_bp.bse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate the intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.4094834484009602, 5.4405165515990408)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b1\n",
    "b1_interv = (lr_bp.params['moisture'] - B_bp*lr_bp.bse['moisture'],\n",
    "             lr_bp.params['moisture'] + B_bp*lr_bp.bse['moisture'])\n",
    "b1_interv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.1042359583483567, 6.6457640416516153)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b2\n",
    "b2_interv = (lr_bp.params['sweetness'] - B_bp*lr_bp.bse['sweetness'],\n",
    "             lr_bp.params['sweetness'] + B_bp*lr_bp.bse['sweetness'])\n",
    "b2_interv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these intervals are wider than the intervals given by `lr_bp.conf_int(alpha=0.01)`, that means `lr_bp.conf_int` is not using the Bonferroni method. Now I want to know what Minitab does:\n",
    "\n",
    "    Coefficients\n",
    "\n",
    "    Term       Coef  SE Coef      99% CI      T-Value  P-Value   VIF\n",
    "    Constant  37.65     3.00  (28.62, 46.68)    12.57    0.000\n",
    "    X1        4.425    0.301  (3.518, 5.332)    14.70    0.000  1.00\n",
    "    X2        4.375    0.673  (2.347, 6.403)     6.50    0.000  1.00\n",
    "\n",
    "It appears that Minitab is also _not_ using Bonferroni. So, just to be clear, the Bonferroni confidence intervals are 3.4094834484009602, 5.4405165515990408) for $\\beta_1$ and (2.1042359583483567, 6.6457640416516153) for $\\beta_2$. \n",
    "\n",
    "Interpretation: if I collect many samples of 16 data points and compute joint confidence intervals for $\\beta_1$ and $\\beta_2$ using a 99 percent family confidence coefficient, 99% of the joint intervals would contain the true values of $\\beta_1$ and $\\beta_2$ simultaneously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7\n",
    "\n",
    "### a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95205897305541431"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bp.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measures the proportionate reduction of total variation in $Y$ associated with the use of the two predictor variables, moisture and sweetness. \n",
    "\n",
    "### b.\n",
    "\n",
    "According to Comment 2 in Kutner, p. 227, the coefficient of multiple determination will be equal to the coefficient of simple determination between $Y_i$ and $\\hat Y_i$, so it is also 0.9521. Confirming with Minitab:\n",
    "\n",
    "    Regression Analysis: Y versus FITS1 \n",
    "\n",
    "    Model Summary\n",
    "\n",
    "          S    R-sq  R-sq(adj)    PRESS  R-sq(pred)\n",
    "    2.59533  95.21%     94.86%  127.759      93.50%\n",
    "\n",
    "Yes, Minitab agrees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8\n",
    "\n",
    "### a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the predict function in Minitab:\n",
    "\n",
    "    Prediction for Y \n",
    "\n",
    "    Variable  Setting\n",
    "    X1              5\n",
    "    X2              4\n",
    "\n",
    "       Fit   SE Fit        99% CI              99% PI\n",
    "    77.275  1.12669  (73.8811, 80.6689)  (68.4808, 86.0692)\n",
    "\n",
    "So, $E\\{Y_h\\} = (73.8811, 80.6689)$ with 99% confidence. If we were to take many samples with a moisture value of 5 and a sweetness value of 4 and constructed a 99% confidence interval for brand liking for each sample, 99% of the intervals would contain the true value of brand liking for moisture of 5 and sweetness of 4. \n",
    "\n",
    "### b. \n",
    "\n",
    "Referrring to the Minitab output above, the 99% confidence prediction interval for brand liking when moisture is 5 and sweetness is 4 is (68.4808, 86.0692). We expect a new observation for moisture = 5 and sweetness = 4 to have a brand liking value within this range, with 99% confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.\n",
    "\n",
    "a. Yes: \n",
    "\n",
    "Let us define:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_{i1} &= X_{i1} \\\\\n",
    "Z_{i2} &= log_{10}(X_{i2}) \\\\\n",
    "Z_{i3} &= X_{i1}^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can then write the equation as:\n",
    "\n",
    "$$ Y_i = \\beta_0 + \\beta_1Z_{i1} + \\beta_2Z_{i2} + \\beta_3Z_{i3} + \\varepsilon_i$$\n",
    "\n",
    "b. No; No\n",
    "\n",
    "c. Yes: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y_i &= log(\\beta_1X_{i1}) + \\beta_2X_{i2}+\\varepsilon_i \\\\\n",
    "Y_i &= log(\\beta_1) + log(X_{i1}) + \\beta_2X_{i2}+\\varepsilon_i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let us define: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_0 &= log(\\beta_1) \\\\\n",
    "\\beta_1 &= 1 \\\\\n",
    "Z_{i1} &= log(X_{i1}) \\\\\n",
    "Z_{i2} &= X_{i2} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can then write the equation as:\n",
    "\n",
    "$$ Y_i = \\beta_0 + \\beta_1Z_{i1} + \\beta_2Z_{i2} + \\varepsilon_i$$\n",
    "\n",
    "d. No; No\n",
    "\n",
    "e. No; Yes: $Y' = ln(\\frac{1}{Y}-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.23\n",
    "\n",
    "### a.\n",
    "\n",
    "Least squares criterion:\n",
    "\n",
    "$$ Q = \\sum_{i=1}^n(Y_i - \\beta_1X_{i1} - \\beta_2X_{i2})^2 \\tag{1}$$\n",
    "\n",
    "The least squares estimators are those values of $\\beta_1$ and $\\beta_2$ that minimize $Q$. To find them, we'll differentiate (1) with respect to $\\beta_1$ and $\\beta_2$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial Q}{\\partial\\beta_1} &= -2\\sum_{i=1}^nX_{i1}(Y_i - \\beta_1X_{i1} - \\beta_2X_{i2}) \\\\\n",
    "\\frac{\\partial Q}{\\partial\\beta_2} &= -2\\sum_{i=1}^nX_{i2}(Y_i - \\beta_1X_{i1} - \\beta_2X_{i2}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We then set these partial derivates equal to zero, using $b_1$ and $b_2$ to denote the least squares estimators of $\\beta_1$ and $\\beta_2$ (the values of $\\beta_1$ and $\\beta_2$ that minimize $Q$):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-2\\sum_{i=1}^nX_{i1}(Y_i - b_1X_{i1} - b_2X_{i2}) &= 0 \\\\\n",
    "-2\\sum_{i=1}^nX_{i2}(Y_i - b_1X_{i1} - b_2X_{i2}) &= 0 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Simplifying and expanding, we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum X_{i1}Y_i - b_1\\sum X_{i1}^2 - b_2\\sum X_{i1}X_{i2} &= 0 \\\\\n",
    "\\sum X_{i2}Y_i - b_1\\sum X_{i1}X_{i2} - b_2\\sum X_{i2}^2 &= 0 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We then solve these equations simultaneously for $b_1$ and $b_2$. Each attempt that I made to solve this system of equations failed. It appears that I might have more success if I used the matrix representation from Kutner et al. p. 241 ($\\mathbf{b=(X'X)^{-1}X'Y}$). However, I have run out of time and Dr. Wang said that I have done enough.\n",
    "\n",
    "\n",
    "### b. \n",
    "\n",
    "Using (6.26), the likelihood function for this model is:\n",
    "\n",
    "$$ L(\\beta,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i - \\beta_1X_{i1} - \\beta_2X_{i2})^2\\right] $$\n",
    "\n",
    "According to Kutner et al. (5th ed., p. 224), maximizing this likelihood function with respect to $\\beta_1$ and $\\beta_2$ leads to maximum likelihood estimators for $\\beta_1$ and $\\beta_2$ that equal the least squares estimators given above. To show this I would follow the same procedure as I did for the least squares estimators above: take partial derivatives of $L$ (actually, of $ln(L)$, as described in Kutner et al. p. 32) with respect to $\\beta_1$ and $\\beta_2$, equating the partials to zero and solving the resulting system of equations for $\\hat \\beta_1$ and $\\hat \\beta_2$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
